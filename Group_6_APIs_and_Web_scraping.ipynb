{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. APIs Intranet task 0"
      ],
      "metadata": {
        "id": "PXFnXukNOrnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "This module provides a function to get a list of starships\n",
        "that can hold a specified number of passengers.\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "\n",
        "\n",
        "def availableShips(passengerCount):\n",
        "    \"\"\"\n",
        "    Retrieve a list of starships that can hold\n",
        "    at least `passengerCount` passengers.\n",
        "\n",
        "    Args:\n",
        "        passengerCount (int): The minimum number of passengers\n",
        "        the starship should be able to hold.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of starship names that can hold\n",
        "        the specified number of passengers.\n",
        "    \"\"\"\n",
        "    url = \"https://swapi-api.alx-tools.com/api/starships/\"\n",
        "    ships = []\n",
        "\n",
        "    while url:\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "\n",
        "        for ship in data[\"results\"]:\n",
        "            passengers = ship[\"passengers\"]\n",
        "            if passengers not in [\"n/a\", \"unknown\", \"0\", \"none\"]:\n",
        "                passengers = passengers.replace(\",\", \"\")\n",
        "                if int(passengers) >= passengerCount:\n",
        "                    ships.append(ship[\"name\"])\n",
        "\n",
        "        url = data[\"next\"]\n",
        "\n",
        "    return ships\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ships = availableShips(4)\n",
        "    for ship in ships:\n",
        "        print(ship)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYZ19Ct6OjCi",
        "outputId": "1325bf6e-bb93-4929-bad4-eac643de4af7"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CR90 corvette\n",
            "Sentinel-class landing craft\n",
            "Death Star\n",
            "Millennium Falcon\n",
            "Executor\n",
            "Rebel transport\n",
            "Slave 1\n",
            "Imperial shuttle\n",
            "EF76 Nebulon-B escort frigate\n",
            "Calamari Cruiser\n",
            "Republic Cruiser\n",
            "Droid control ship\n",
            "Scimitar\n",
            "J-type diplomatic barge\n",
            "AA-9 Coruscant freighter\n",
            "Republic Assault ship\n",
            "Solar Sailer\n",
            "Trade Federation cruiser\n",
            "Theta-class T-2c shuttle\n",
            "Republic attack cruiser\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Scraping the tabular data"
      ],
      "metadata": {
        "id": "oA5xxOSFM-iq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26CgSflHdt2L",
        "outputId": "ec660e55-65d9-40d2-98ff-cd46013e911b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                Team Name  Year Wins Losses OT Losses  Win % Goals For (GF)  \\\n",
            "0           Boston Bruins  1990   44     24             0.55            299   \n",
            "1          Buffalo Sabres  1990   31     30            0.388            292   \n",
            "2          Calgary Flames  1990   46     26            0.575            344   \n",
            "3      Chicago Blackhawks  1990   49     23            0.613            284   \n",
            "4       Detroit Red Wings  1990   34     38            0.425            273   \n",
            "5         Edmonton Oilers  1990   37     37            0.463            272   \n",
            "6        Hartford Whalers  1990   31     38            0.388            238   \n",
            "7       Los Angeles Kings  1990   46     24            0.575            340   \n",
            "8   Minnesota North Stars  1990   27     39            0.338            256   \n",
            "9      Montreal Canadiens  1990   39     30            0.487            273   \n",
            "10      New Jersey Devils  1990   32     33              0.4            272   \n",
            "11     New York Islanders  1990   25     45            0.312            223   \n",
            "12       New York Rangers  1990   36     31             0.45            297   \n",
            "13    Philadelphia Flyers  1990   33     37            0.412            252   \n",
            "14    Pittsburgh Penguins  1990   41     33            0.512            342   \n",
            "15       Quebec Nordiques  1990   16     50              0.2            236   \n",
            "16        St. Louis Blues  1990   47     22            0.588            310   \n",
            "17    Toronto Maple Leafs  1990   23     46            0.287            241   \n",
            "18      Vancouver Canucks  1990   28     43             0.35            243   \n",
            "19    Washington Capitals  1990   37     36            0.463            258   \n",
            "20          Winnipeg Jets  1990   26     43            0.325            260   \n",
            "21          Boston Bruins  1991   36     32             0.45            270   \n",
            "22         Buffalo Sabres  1991   31     37            0.388            289   \n",
            "23         Calgary Flames  1991   31     37            0.388            296   \n",
            "24     Chicago Blackhawks  1991   36     29             0.45            257   \n",
            "\n",
            "   Goals Against (GA) + / -  \n",
            "0                 264    35  \n",
            "1                 278    14  \n",
            "2                 263    81  \n",
            "3                 211    73  \n",
            "4                 298   -25  \n",
            "5                 272     0  \n",
            "6                 276   -38  \n",
            "7                 254    86  \n",
            "8                 266   -10  \n",
            "9                 249    24  \n",
            "10                264     8  \n",
            "11                290   -67  \n",
            "12                265    32  \n",
            "13                267   -15  \n",
            "14                305    37  \n",
            "15                354  -118  \n",
            "16                250    60  \n",
            "17                318   -77  \n",
            "18                315   -72  \n",
            "19                258     0  \n",
            "20                288   -28  \n",
            "21                275    -5  \n",
            "22                299   -10  \n",
            "23                305    -9  \n",
            "24                236    21  \n",
            "Data has been successfully written to scraped_table_data.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# URL of the page to scrape\n",
        "url = 'https://www.scrapethissite.com/pages/forms/'\n",
        "\n",
        "# Request to fetch the page content\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Finding the table element\n",
        "table = soup.find('table')\n",
        "\n",
        "# Extract table headers\n",
        "headers = [header.text.strip() for header in table.find_all('th')]\n",
        "\n",
        "# Extract table rows\n",
        "rows = []\n",
        "for row in table.find_all('tr')[1:]:  # Skip the header row\n",
        "    cols = [col.text.strip() for col in row.find_all('td')]\n",
        "    if cols:\n",
        "        rows.append(cols)\n",
        "\n",
        "# Create a DataFrame from the extracted data\n",
        "df = pd.DataFrame(rows, columns=headers)\n",
        "\n",
        "# Display the table\n",
        "print(df)\n",
        "\n",
        "\n",
        "# Saved data to a CSV file\n",
        "csv_file = 'scraped_table_data.csv'\n",
        "with open(csv_file, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(headers)\n",
        "    writer.writerows(rows)\n",
        "\n",
        "print(f\"Data has been successfully written to {csv_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Scraping 5 products of different categories from Amazon.com"
      ],
      "metadata": {
        "id": "cnVbL3C6NI-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "# List of user agents to choose from\n",
        "USER_AGENTS = [\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0.2 Safari/605.1.15',\n",
        "    'Mozilla/5.0 (Linux; Android 10; Pixel 3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Mobile Safari/537.36',\n",
        "    'Mozilla/5.0 (Linux; Android 11; SM-G950U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Mobile Safari/537.36',\n",
        "]\n",
        "\n",
        "# List of categories and their respective URLs\n",
        "categories = {\n",
        "    'laptops': 'https://www.amazon.com/s?k=laptop&crid=69J5LAQPRTNY&sprefix=laptop%2Caps%2C326&ref=nb_sb_ss_ts-doa-p_1_6',\n",
        "    'books': 'https://www.amazon.com/s?k=books&i=stripbooks-intl-ship&crid=1SH8JGDPMX8JZ&sprefix=book%2Cstripbooks-intl-ship%2C318&ref=nb_sb_noss_1',\n",
        "    'shoes': 'https://www.amazon.com/s?k=shoes&i=fashion-boys-intl-ship&crid=JR3YXOQQ070X&sprefix=shoe%2Cfashion-boys-intl-ship%2C303&ref=nb_sb_noss_2',\n",
        "    'movies': 'https://www.amazon.com/s?k=movies&i=movies-tv-intl-ship&crid=2WL62AUHVUHN8&sprefix=mov%2Cmovies-tv-intl-ship%2C302&ref=nb_sb_noss_2',\n",
        "    'cookers': 'https://www.amazon.com/s?k=cooker&ref=nb_sb_noss'\n",
        "}\n",
        "\n",
        "# Function to create a directory to save images\n",
        "def create_dir(dir_name):\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.makedirs(dir_name)\n",
        "\n",
        "# Function to scrape Amazon products\n",
        "def scrape_amazon_products(url):\n",
        "    # Select a random user agent\n",
        "    headers = {\n",
        "        'User-Agent': random.choice(USER_AGENTS),\n",
        "        'Accept-Language': 'en-US,en;q=0.5',\n",
        "        'Accept-Encoding': 'gzip, deflate, br',\n",
        "        'Connection': 'keep-alive',\n",
        "        'Upgrade-Insecure-Requests': '1',\n",
        "        'Referer': 'https://www.amazon.com/'\n",
        "    }\n",
        "\n",
        "    # Sleep before making the request to mimic human behavior\n",
        "    time.sleep(random.uniform(2, 5))\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # Raise an error for bad responses\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Failed to retrieve the webpage: {e}\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find product listings\n",
        "    products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
        "\n",
        "    # List to hold product data\n",
        "    product_data = []\n",
        "\n",
        "    # Loop through each product and gather data\n",
        "    for product in products:\n",
        "        title_tag = product.h2\n",
        "        if title_tag:\n",
        "            title = title_tag.text.strip()\n",
        "            image_tag = product.find('img')\n",
        "            if image_tag and 'src' in image_tag.attrs:\n",
        "                image = image_tag['src']\n",
        "                product_data.append((title, image))\n",
        "                if len(product_data) >= 5:  # Limit to first 5 products\n",
        "                    break\n",
        "\n",
        "    return product_data\n",
        "\n",
        "# Function to save images in respective category folders\n",
        "def save_images(category, product_data):\n",
        "    create_dir(f'amazon_images/{category}')  # Create category folder\n",
        "\n",
        "    for title, image_url in product_data:\n",
        "        try:\n",
        "            image_response = requests.get(image_url)\n",
        "            image_response.raise_for_status()  # Raise an error for bad responses\n",
        "            image_name = title.replace(\" \", \"_\").replace(\"/\", \"_\") + '.jpg'\n",
        "\n",
        "            with open(os.path.join(f'amazon_images/{category}', image_name), 'wb') as img_file:\n",
        "                img_file.write(image_response.content)\n",
        "\n",
        "            print(f'Saved image: {image_name} in category: {category}')\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Failed to save image for '{title}': {e}\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    for category, url in categories.items():\n",
        "        print(f'Scraping category: {category}')\n",
        "        products = scrape_amazon_products(url)\n",
        "        if products:\n",
        "            save_images(category, products)\n",
        "        else:\n",
        "            print(f\"No products found in category: {category}\")\n"
      ],
      "metadata": {
        "id": "eiWjunNXV_3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2ba7125-ba5c-4443-db95-7fa0a4373ac9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping category: laptops\n",
            "Saved image: Acer_Aspire_3_A315-24P-R7VH_Slim_Laptop_|_15.6\"_Full_HD_IPS_Display_|_AMD_Ryzen_3_7320U_Quad-Core_Processor_|_AMD_Radeon_Graphics_|_8GB_LPDDR5_|_128GB_NVMe_SSD_|_Wi-Fi_6_|_Windows_11_Home_in_S_Mode.jpg in category: laptops\n",
            "Saved image: HP_Newest_255_G10_Laptop_for_Home_or_Work,_16GB_RAM,_1TB_SSD,_15.6\"_Full_HD,_Ryzen_3_7330U_(Beat_Intel_i5-1135G7),_Ethernet_Port,_HDMI,_USB-C,_Windows_11_Pro,_Business_and_Fun_Ready_(2024).jpg in category: laptops\n",
            "Saved image: HP_Newest_14\"_Ultral_Light_Laptop_for_Students_and_Business,_Intel_Quad-Core_N4120,_8GB_RAM,_192GB_Storage(64GB_eMMC+128GB_Micro_SD),_1_Year_Office_365,_Webcam,_HDMI,_WiFi,_USB-A&C,_Win_11_S.jpg in category: laptops\n",
            "Saved image: Acer_Aspire_Go_15_Slim_Laptop_|_15.6\"_Full_HD_IPS_1080P_Display_|_Intel_Core_i3-N305|_Intel_UHD_Graphics_|_8GB_LPDDR5_|_128GB_HD_|_Wi-Fi_6_|_AI_PC_|_Windows_11_Home_in_S_Mode_|_AG15-31P-3947.jpg in category: laptops\n",
            "Saved image: HP_14_Laptop,_Intel_Celeron_N4020,_4_GB_RAM,_64_GB_Storage,_14-inch_Micro-edge_HD_Display,_Windows_11_Home,_Thin_&_Portable,_4K_Graphics,_One_Year_of_Microsoft_365_(14-dq0040nr,_Snowflake_White).jpg in category: laptops\n",
            "Scraping category: books\n",
            "Saved image: Melania.jpg in category: books\n",
            "Saved image: War.jpg in category: books\n",
            "Saved image: From_Here_to_the_Great_Unknown:_A_Memoir.jpg in category: books\n",
            "Saved image: Good_Energy:_The_Surprising_Connection_Between_Metabolism_and_Limitless_Health.jpg in category: books\n",
            "Saved image: Onyx_Storm_(Deluxe_Limited_Edition)_(The_Empyrean,_3).jpg in category: books\n",
            "Scraping category: shoes\n",
            "Saved image: Under_Armour.jpg in category: shoes\n",
            "Saved image: Under_Armour.jpg in category: shoes\n",
            "Saved image: Skechers.jpg in category: shoes\n",
            "Saved image: PUMA.jpg in category: shoes\n",
            "Saved image: koppu.jpg in category: shoes\n",
            "Scraping category: movies\n",
            "Saved image: Beetlejuice_Beetlejuice_(Bonus_X-Ray_Edition).jpg in category: movies\n",
            "Saved image: Deadpool_&_Wolverine_(Bonus_X-Ray_Edition).jpg in category: movies\n",
            "Saved image: Despicable_Me_4_(Includes_2_Mini-Movies).jpg in category: movies\n",
            "Saved image: Brothers.jpg in category: movies\n",
            "Saved image: Twisters.jpg in category: movies\n",
            "Scraping category: cookers\n",
            "Saved image: Instant_Pot_Duo_7-in-1_Mini_Electric_Pressure_Cooker,_Slow_Rice_Cooker,_Steamer,_Sauté,_Yogurt_Maker,_Warmer_&_Sterilizer,_Includes_Free_App_with_over_1900_Recipes,_Stainless_Steel,_3_Quart.jpg in category: cookers\n",
            "Saved image: Crock-Pot_7_Quart_Oval_Manual_Slow_Cooker,_Stainless_Steel_(SCV700-S-BR),_Versatile_Cookware_for_Large_Families_or_Entertaining.jpg in category: cookers\n",
            "Saved image: Hawkins_CB30_Hard_Anodised_Pressure_Cooker,_3-Liter,_Contura_Black.jpg in category: cookers\n",
            "Saved image: HAWKINS_Classic_CL50_5-Liter_New_Improved_Aluminum_Pressure_Cooker,_Small,_Silver.jpg in category: cookers\n",
            "Saved image: AROMA_Digital_Rice_Cooker,_4-Cup_(Uncooked)___8-Cup_(Cooked),_Steamer,_Grain_Cooker,_Multicooker,_2_Qt,_Stainless_Steel_Exterior,_ARC-914SBD.jpg in category: cookers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r amazon_images.zip amazon_images\n",
        "from google.colab import files\n",
        "files.download('amazon_images.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "swPYKP0yM2Np",
        "outputId": "95acb657-d19e-42fd-8238-9d9ad326c4de"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: amazon_images/ (stored 0%)\n",
            "  adding: amazon_images/books/ (stored 0%)\n",
            "  adding: amazon_images/books/From_Here_to_the_Great_Unknown:_A_Memoir.jpg (stored 0%)\n",
            "  adding: amazon_images/books/Melania.jpg (deflated 15%)\n",
            "  adding: amazon_images/books/Good_Energy:_The_Surprising_Connection_Between_Metabolism_and_Limitless_Health.jpg (stored 0%)\n",
            "  adding: amazon_images/books/Onyx_Storm_(Deluxe_Limited_Edition)_(The_Empyrean,_3).jpg (deflated 0%)\n",
            "  adding: amazon_images/books/War.jpg (deflated 1%)\n",
            "  adding: amazon_images/cookers/ (stored 0%)\n",
            "  adding: amazon_images/cookers/Hawkins_CB30_Hard_Anodised_Pressure_Cooker,_3-Liter,_Contura_Black.jpg (deflated 0%)\n",
            "  adding: amazon_images/cookers/Instant_Pot_Duo_7-in-1_Mini_Electric_Pressure_Cooker,_Slow_Rice_Cooker,_Steamer,_Sauté,_Yogurt_Maker,_Warmer_&_Sterilizer,_Includes_Free_App_with_over_1900_Recipes,_Stainless_Steel,_3_Quart.jpg (stored 0%)\n",
            "  adding: amazon_images/cookers/HAWKINS_Classic_CL50_5-Liter_New_Improved_Aluminum_Pressure_Cooker,_Small,_Silver.jpg (stored 0%)\n",
            "  adding: amazon_images/cookers/Crock-Pot_7_Quart_Oval_Manual_Slow_Cooker,_Stainless_Steel_(SCV700-S-BR),_Versatile_Cookware_for_Large_Families_or_Entertaining.jpg (stored 0%)\n",
            "  adding: amazon_images/cookers/AROMA_Digital_Rice_Cooker,_4-Cup_(Uncooked)___8-Cup_(Cooked),_Steamer,_Grain_Cooker,_Multicooker,_2_Qt,_Stainless_Steel_Exterior,_ARC-914SBD.jpg (deflated 0%)\n",
            "  adding: amazon_images/laptops/ (stored 0%)\n",
            "  adding: amazon_images/laptops/HP_Newest_255_G10_Laptop_for_Home_or_Work,_16GB_RAM,_1TB_SSD,_15.6\"_Full_HD,_Ryzen_3_7330U_(Beat_Intel_i5-1135G7),_Ethernet_Port,_HDMI,_USB-C,_Windows_11_Pro,_Business_and_Fun_Ready_(2024).jpg (deflated 1%)\n",
            "  adding: amazon_images/laptops/Acer_Aspire_Go_15_Slim_Laptop_|_15.6\"_Full_HD_IPS_1080P_Display_|_Intel_Core_i3-N305|_Intel_UHD_Graphics_|_8GB_LPDDR5_|_128GB_HD_|_Wi-Fi_6_|_AI_PC_|_Windows_11_Home_in_S_Mode_|_AG15-31P-3947.jpg (deflated 2%)\n",
            "  adding: amazon_images/laptops/HP_14_Laptop,_Intel_Celeron_N4020,_4_GB_RAM,_64_GB_Storage,_14-inch_Micro-edge_HD_Display,_Windows_11_Home,_Thin_&_Portable,_4K_Graphics,_One_Year_of_Microsoft_365_(14-dq0040nr,_Snowflake_White).jpg (deflated 1%)\n",
            "  adding: amazon_images/laptops/HP_Newest_14\"_Ultral_Light_Laptop_for_Students_and_Business,_Intel_Quad-Core_N4120,_8GB_RAM,_192GB_Storage(64GB_eMMC+128GB_Micro_SD),_1_Year_Office_365,_Webcam,_HDMI,_WiFi,_USB-A&C,_Win_11_S.jpg (deflated 2%)\n",
            "  adding: amazon_images/laptops/Acer_Aspire_3_A315-24P-R7VH_Slim_Laptop_|_15.6\"_Full_HD_IPS_Display_|_AMD_Ryzen_3_7320U_Quad-Core_Processor_|_AMD_Radeon_Graphics_|_8GB_LPDDR5_|_128GB_NVMe_SSD_|_Wi-Fi_6_|_Windows_11_Home_in_S_Mode.jpg (deflated 2%)\n",
            "  adding: amazon_images/shoes/ (stored 0%)\n",
            "  adding: amazon_images/shoes/Skechers.jpg (deflated 1%)\n",
            "  adding: amazon_images/shoes/koppu.jpg (deflated 1%)\n",
            "  adding: amazon_images/shoes/PUMA.jpg (deflated 1%)\n",
            "  adding: amazon_images/shoes/Under_Armour.jpg (deflated 1%)\n",
            "  adding: amazon_images/movies/ (stored 0%)\n",
            "  adding: amazon_images/movies/Twisters.jpg (stored 0%)\n",
            "  adding: amazon_images/movies/Beetlejuice_Beetlejuice_(Bonus_X-Ray_Edition).jpg (stored 0%)\n",
            "  adding: amazon_images/movies/Deadpool_&_Wolverine_(Bonus_X-Ray_Edition).jpg (stored 0%)\n",
            "  adding: amazon_images/movies/Brothers.jpg (stored 0%)\n",
            "  adding: amazon_images/movies/Despicable_Me_4_(Includes_2_Mini-Movies).jpg (stored 0%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cc9fd8c7-a992-4af3-82d2-ec178a58b85f\", \"amazon_images.zip\", 240326)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
